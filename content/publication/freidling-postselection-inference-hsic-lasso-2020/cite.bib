@article{freidlingPostselectionInferenceHSICLasso2020,
 abstract = {Detecting inﬂuential features in complex (non-linear and/or high-dimensional) datasets is key for extracting the relevant information. Most of the popular selection procedures, however, require assumptions on the underlying data - such as distributional ones -, which barely agree with empirical observations. Therefore, feature selection based on nonlinear methods, such as the model-free HSIC-Lasso, is a more relevant approach. In order to ensure valid inference among the chosen features, the selection procedure must be accounted for. In this paper, we propose selective inference with HSIC-Lasso using the framework of truncated Gaussians together with the polyhedral lemma. Based on these theoretical foundations, we develop an algorithm allowing for low computational costs and the treatment of the hyper-parameter selection issue. The relevance of our method is illustrated using artiﬁcial and real-world datasets. In particular, our empirical ﬁndings emphasise that type-I error control at the considered level can be achieved.},
 author = {Freidling, Tobias and Poignard, Benjamin and Climente-González, Héctor and Yamada, Makoto},
 copyright = {All rights reserved},
 file = {2020_Freidling_et_al_arXiv2010.15659_[math,_stat]_Post-selection_inference_with_HSIC-Lasso.pdf:/Users/hclimente/Documents/papers/2020_Freidling_et_al_arXiv2010.15659_[math,_stat]_Post-selection_inference_with_HSIC-Lasso.pdf:application/pdf},
 journal = {arXiv:2010.15659 [math, stat]},
 keywords = {Mathematics - Statistics Theory, Statistics - Machine Learning},
 language = {en},
 month = {October},
 note = {00000 
arXiv: 2010.15659},
 title = {Post-selection inference with HSIC-Lasso},
 url = {http://arxiv.org/abs/2010.15659},
 urldate = {2021-02-25},
 year = {2020}
}

